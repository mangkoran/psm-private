\documentclass[../index.tex]{subfiles}

\begin{document}

\chapter{IMPLEMENTATION AND TESTING}

This chapter will explain the system implementation which includes the configuration code of the
proposed system that were carried out as continuation from design and analysis stage. This stage is
exceedingly critical for the project to achieve the fulfillment the system requirements. 

Both black-box and white-box testing are employed to ensure the functionality and system internals
are operating and behaving correctly to meet the system requirements.

\section{Main System Functionality Code}

In this section, the main functionality of the proposed system implementation will be explained.
Code snippet of the relevant functionality is attached to help visualizing the programming logic
involved.

\subsection{Proxmox VE}

Proxmox VE initial setup involves manual installation due to Proxmox role as the foundation of the
proposed system. Proxmox installation requires a USB drive which will be used to boot into Proxmox
installation image.

\subsection{OPNSense}

Figure ... shows the Ansible playbook and task to bootstrap a KVM virtual machine that will host
OPNSense firewall. The Ansible playbook will firstly check whether a KVM virtual machine with the
name opnsense1 is already existed. If there is no such virtual machine, the Ansible playbook will
create a new KVM virtual machine. As with any KVM virtual machine, manual installation procedure is
required for the initial OPNSense setup by booting the virtual machine with OPNSense installation
image ISO file. The initial setup will prompt for network interface assignment.

After the OPNSense network interface has been configured, the OPNSense web GUI can be accessed from
OPNSense LAN IP address to configure API keys for management through Ansible. By using the API keys,
several OPNSense module can be configured through Ansible. In the proposed system, the Ansible
playbook will be used to configure firewall rules, Suricata intrusion detection system, Telegraf
metric collector and forwarder agent, and syslog forwarding targets. 

\subsubsection{Firewall}

...

\subsubsection{Suricata}

Figure ... shows the Ansible playbook and task to enable Suricata. Suricata is a network intrusion
detection system maintained by Open Information Security Foundation (OISF). In the proposed system,
Suricata is used to watch local network traffic pattern and trigger alert if a pattern matches known
suspicious traffic pattern.

\subsubsection{Telegraf}

Figure ... is the Ansible playbook and task to enable and configure Telegraf. Telegraf is ...

\subsection{Pi-hole}

% \begin{figure}[h]
%    \tcbmintedinput{yaml}{../pve-ansible/pve/pihole.yml}
%    \caption{Pi-hole Ansible playbook}
% \end{figure}

\codesnippet{yaml}{../pve-ansible/pve/pihole.yml}{Pi-hole Ansible playbook}


Figure ... shows the Ansible playbook and task to bootstrap an Ubuntu LXC container that will host
Pi-hole DNS server. The Ansible playbook will firstly check whether an LXC container with the name
pihole1 is already existed. If there is no such container, the Ansible playbook will create a new
Ubuntu container.

If an LXC container with the name pihole1 is existed, the Ansible playbook will check whether pihole
program is already installed. If there is no pihole program installed, the Ansible playbook will
proceed to install Pi-hole using officially provided install script. Before the installation, the
Ansible playbook will ensure the required dependencies are installed. Followed by that, the Ansible
playbook will copy the unattended setup configuration file from the control node to the pihole1
container. After both the dependencies and setup configuration exist, the Ansible playbook will
install pihole using the install script.

In addition to the Pi-hole DNS server, the Ansible playbook will configure Rsyslog so that any DNS
action log produced by Pi-hole will be forwarded to the monitoring1 container which will be
processed and sanitized by Vector.

\subsection{Monitoring}

Figure ... shows the Ansible playbook and task to bootstrap an Ubuntu LXC container that will host
the monitoring stack of the system. The monitoring stack consists of Vector, Loki, Prometheus, and
Grafana. The ansible playbook will firstly check whether an LXC container with the name monitoring1
is already existed. If there is no such container, the Ansible playbook will create a new Ubuntu
container.

If an LXC container with the name monitoring1 is existed, the Ansible playbook will check whether
the monitoring stack programs are already installed, which are Vector, Loki, Prometheus, and
Grafana. If there are not any of the monitoring stack programs, the Ansible playbook will proceed to
install the missing programs through apt package manager. In addition, Vector and Grafana apt
repository will be configured before Vector, Loki, and Grafana can be installed through apt package
manager. Followed by the that, the Ansible playbook will copy the monitoring stack configuration
files from the control node to the monitoring1. After the configuration files copied to their
respective path, the respective program systemd service will be restarted or reloaded to make them
load the new configuration.

\subsubsection{Vector}

Figure ... shows the Ansible playbook and task to install and configure Vector. Vector is a data
pipeline which allow user to collect, transform, and route any log, metric, and traces to various
sinks. In the proposed system, Vector is used to process syslog data received from opnsense1 virtual
machine and pihole1 container.

Vector will filter the received syslog based on their appname. In this case, Vector will filter and
segregate the syslog to filterlog, suricata, and pihole. Afterwards, Vector will do field remapping
which sanitize the log to only includes useful information. For filterlog log, Vector will enrich
the filterlog with GeoIP information, embedding additional geolocation data to the filterlog.
Lastly, Vector will ship the processed log to Loki for log storage and query.

\subsubsection{Loki}

Figure ... shows the Ansible playbook and task to install and configure Loki. Grafana Loki is a log
aggregation system in Grafana Open Source stack which allow user to store and query application log.
In the proposed system, Loki is used to store processed syslog from Vector. Afterwards, these stored
log will act as data source for Grafana dasboard, allowing log visualization through LogQL query
language.

\subsubsection{Prometheus}

Prometheus is a system monitoring and alerting toolkit which allow appplication and system metric
collection and storage. In the proposed system, Prometheus is used to store system metric of all
virtual machines and containers in the system, which include opnsense1 virtual machine, pihole1
container, and monitoring1 container. Telegraf will scrape several system metric, such as ...

This system metric  will be collected and stored by Prometheus. The stored metrics will act as data
source for Grafana dashboard for metric visualization through PromQL query language.

\subsubsection{Grafana}

Grafana is a data visualization platform which allow user to make query, create visualization, and
setup alert of system metric, event log, and application trace. In the proposed system, Grafana is
used to create visualization dashboard of firewall event, DNS query log, IPS action, and virtual
machine and containers system metric.

\section{System Testing}

In Testing section, several system main functionality will be examined for the purpose of ensuring
the system capabilities to fulfill the project requirements. In the proposed system, both black-box
and white-box testing is utilized with each having different purpose.

\subsection{Black-box Testing}

Black-box testing ...

\subsection{White-box Testing}

White-box testing ...

% https://blog.dmcindoe.dev/posts/2021-07-31/automating-proxmox-with-ansible/

\end{document}
